{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzwJ619dAlA5"
   },
   "outputs": [],
   "source": [
    "import numpy as np, os\n",
    "if not [int(i) for i in np.__version__.split('.')] >= [1,20,1]: # check numpy version\n",
    "    print('Upgrading numpy and stopping RUNTIME! When the notebook completes, please run again.')\n",
    "    ! pip install --upgrade numpy    # suggested by Efosa, see also https://github.com/jswhit/pygrib/issues/192\n",
    "    os.kill(os.getpid(), 9)          # kill the runtime, need to run again from the beginning! pip install pygrib\n",
    "! pip install pygrib   \n",
    "from grib_file import GribFile     # Martin's utility layer on top of  pygrib,from wrfxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9rvlymMZdJg"
   },
   "source": [
    "## Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPgTHlCLAlA-"
   },
   "source": [
    "The Kalman filter provides an estimate $u$ of the time evolution of some unknown process, called \"nature\" or \"truth\". We do not know with certainty what the nature is, but we can observe it at regular intervals (steps) with some error. In each step, model $F$ advances the model state $u$ in time, $ u \\leftarrow F(u)$, and attempts to reconcile the state with an observation $d$ of the true state, so $u \\approx d$. The filter modifies the model state $u$ to balance the uncertainty in the model and the data (this is called *analysis*) and the cycle continues. For that purpose, the filter evolves also an estimate of the uncertainly of the model.\n",
    "\n",
    "More generally, instead of $u \\approx d$, only a part of the state is observed, and $Hu \\approx d$ where $H$ is a matrix, or observation function. Basically, $Hu$ is what the data would be if the model was completely accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bvUtJ_OLwQA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def ext_kf(u,P,F,Q=0,d=None,H=None,R=None):\n",
    "  \"\"\"\n",
    "  One step of the extended Kalman filter. \n",
    "  If there is no data, only advance in time.\n",
    "  :param u:   the state vector, shape n\n",
    "  :param P:   the state covariance, shape (n,n)\n",
    "  :param Q:   the process model noise covariance, shape (n,n)\n",
    "  :param F:   the model function, maps vector u to vector F(u) and Jacobian J(u)\n",
    "  :param d:   data vector, shape (m)\n",
    "  :param H:   observation matrix, shape (m,n)\n",
    "  :param R:   data error covariance, shape (n,n)\n",
    "  :return ua: the analysis state vector, shape (n)\n",
    "  :return Pa: the analysis covariance matrix, shape (n,n)\n",
    "  \"\"\"\n",
    "  def d2(a):\n",
    "    return np.atleast_2d(a) # convert to at least 2d array\n",
    "\n",
    "  def d1(a):\n",
    "    return np.atleast_1d(a) # convert to at least 1d array\n",
    "\n",
    "  # forecast\n",
    "  uf, J  = F(u)          # advance the model state in time and get the Jacobian\n",
    "  uf = d1(uf)            # if scalar, make state a 1D array\n",
    "  P = d2(P)              # if scalar, make Jacobian as 2D array\n",
    "  Pf  = d2(J.T @ P) @ J + Q  # advance the state covariance Pf = J' * P * J + Q\n",
    "  # analysis\n",
    "  if d is None or not d.size :  # no data, no analysis\n",
    "    return uf, Pf\n",
    "  # K = P H' * inverse(H * P * H' + R) = (inverse(H * P * H' + R)*(H P))'\n",
    "  H = d2(H)\n",
    "  HP  = d2(H @ P)            # precompute a part used twice  \n",
    "  K   = d2(np.linalg.solve( d2(HP @ H.T) + R, HP)).T  # Kalman gain\n",
    "  # print('H',H)\n",
    "  # print('K',K)\n",
    "  res = d1(H @ d1(uf) - d)          # res = H*uf - d\n",
    "  ua = uf - K @ res # analysis mean uf - K*res\n",
    "  Pa = Pf - K @ d2(H @ P)        # analysis covariance\n",
    "  return ua, d2(Pa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9pD9grsAJMq"
   },
   "source": [
    "##  A basic exponential decay model of fuel moisture\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHGMoaVWao89"
   },
   "source": [
    "The evolution of fuel moisture content $m(t)$ is modeled by the differential equation on interval $\\left[\n",
    "t_{0},t_{1}\\right]  $,\n",
    "$$\n",
    "\\frac{dm}{dt}=\\frac{E-m(t)}{T},\\quad m(t_{0})=m_{0}.\n",
    "$$\n",
    "where the initial fuel moisture content $m_{0}=m\\left(  t_{0}\\right)  $ is the\n",
    "input, and $m_{1}=m(t_{1})$ is the output. Tnus, $m_1=F(m_0)$. The parameters of the model are the\n",
    "fuel moisture equilibrium $E$, assumed to be constant over the interval $\\left[\n",
    "t_{0},t_{1}\\right]  $, NS the characteristic decay time $T$. \n",
    "\n",
    "We can build the general model later by calling this simple model with different\n",
    "equilibria and time constants (drying, wetting, rain).\n",
    "\n",
    "Since $E$ is constant in time, the solution can be found\n",
    "analytically,\n",
    "$$\n",
    "m\\left(  t\\right)  =E+\\left(  m_{0}-E\\right)  e^{-t/T}%\n",
    "$$\n",
    "For convenience, we use $T_{1}=1/T$ instead of $T$, and the model becomes\n",
    "$$\n",
    "m_{1}=E+\\left(  m_{0}-E\\right)  e^{-\\left(  t_{1}-t_{0}\\right)  T_{1}}%\n",
    "$$\n",
    "In the extended Kalman filter, we will need the partial derivatives of $m_{1}$\n",
    "with respect to the input and the parameters. Compute\n",
    "$$\n",
    "\\frac{dm_{1}}{d_{m0}}=e^{-\\left(  t_{1}-t_{0}\\right)  T_{1}}\n",
    "$$\n",
    "$$\n",
    "\\frac{dm_{1}}{dE}=1-e^{-\\left(  t_{1}-t_{0}\\right)  T_{1}}\n",
    "$$\n",
    "$$\n",
    "\\frac{dm_{1}}{dT_{1}}=-\\left(  m_{0}-E\\right)  \\left(  t_{1}-t_{0}\\right)\n",
    "e^{-\\left(  t_{1}-t_{0}\\right)  T_{1}}\n",
    "$$\n",
    "At the moment, we need only ${dm_{1}}/{dm_{0}}$ but we put in the code all partials for possible use in future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqcs0zEiAn0j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def model_decay(m0,E,partials=0,T1=0.1,tlen=1):  # T1 = 1/T, partials=0 none, 1=dm0/dm1, 2=dm1/dm0, dm1/dE, dm1/dT1   \n",
    "  exp_t = np.exp(-tlen*T1)                  # compute this subexpression only once\n",
    "  m1 = E + (m0 - E)*exp_t                   # the solution at end\n",
    "  if partials==0:\n",
    "    return m1\n",
    "  dm1_dm0 = exp_t\n",
    "  if partials==1:\n",
    "    return m1, np.array([dm1_dm0])          # return value and Jacobian\n",
    "  dm1_dE = 1 - exp_t                        # partial derivative dm1 / dE\n",
    "  dm1_dT1 = -(m0 - E)*tlen*exp_t            # partial derivative dm1 / dT1\n",
    "  return m1, dm1_dm0, dm1_dE, dm1_dT1       # return value and all partial derivatives wrt m1 and parameters\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLPJT3FcA2a7"
   },
   "source": [
    "## Kalman filter demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIA3X8vluFdd"
   },
   "source": [
    "We demonstrate the Kalman filter for this model on a simple artificial example. The model is solving the differential equation for one hour. The equilibrium $E$ is constant during the hour, but it changes over the day so that it is higher at night and lower during the day, with a 24-hour period.  First, we create the \"truth\" by choosing the equilibrium $E$ and solving the differential aquation every hour, with a small additive noise. The synthetic data is obtained as values of the \"truth\", with random noise to simulate observation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBv10PTiChhm"
   },
   "source": [
    "### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_pz-wXnCMnP"
   },
   "outputs": [],
   "source": [
    "import numpy as np, random\n",
    "days = 20       \n",
    "hours = days*24\n",
    "h2 = int(hours/2)\n",
    "hour = range(hours)\n",
    "day = np.array(range(hours))/24.\n",
    "\n",
    "# artificial equilibrium data\n",
    "E = np.power(np.sin(np.pi*day),4) # diurnal curve\n",
    "E = 0.05+0.25*E\n",
    "# FMC free run\n",
    "m_f = np.zeros(hours)\n",
    "m_f[0] = 0.1         # initial FMC\n",
    "for t in range(hours-1):\n",
    "  m_f[t+1] = max(0.,model_decay(m_f[t],E[t])  + random.gauss(0,0.005) )\n",
    "data = m_f + np.random.normal(loc=0,scale=0.02,size=hours)    \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(hour,E,linestyle='--',c='r',label='Equilibrium')\n",
    "plt.plot(hour,m_f,linestyle='-',c='k',label='10-h fuel truth')\n",
    "plt.scatter(hour[:h2],data[:h2],c='b',label='10-h fuel data')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-3WLAEpD2yJ"
   },
   "source": [
    "### Run Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4g-RrrYAlBD"
   },
   "source": [
    "We have used the same code for model and for the truth, and run the Kalman filter for 10 days. The graph below shows that the model state was remarkably close to the truth, even if the model is fed only noisy observations. This is because the dynamics of the model and of the truth are the same. After 10 days, we let the model continue without any new data to simulate forecasting the future, and the agreement with the truth was still very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-CjONZkD18n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def kf_example(DeltaE):\n",
    "  h2 = int(hours/2)\n",
    "  m = np.zeros(hours)\n",
    "  m[0]=0.1             # background state  \n",
    "  P = np.zeros(hours)\n",
    "  P[0] = 0.03 # background state variance\n",
    "  Q = np.array([0.02]) # process noise variance\n",
    "  H = np.array([1.])   # all observed\n",
    "  R = np.array([0.02]) # data variance\n",
    "\n",
    "  for t in range(h2):\n",
    "    # use lambda construction to pass additional arguments to the model \n",
    "    m[t+1],P[t+1] = ext_kf(m[t],P[t],lambda u: model_decay(u,E[t]+DeltaE,partials=1),Q,\n",
    "                    d=data[t],H=H,R=R)\n",
    "  for t in range(h2,hours - 1):\n",
    "    m[t+1],P[t+1] = ext_kf(m[t],P[t],lambda u: model_decay(u,E[t]+DeltaE,partials=1))\n",
    "  \n",
    "  %matplotlib inline\n",
    "  plt.figure(figsize=(16,4))\n",
    "  plt.plot(hour,E,linestyle='--',c='r',label='Equilibrium')\n",
    "  print(len(hour),len(m_f))\n",
    "  plt.plot(hour,m_f,linestyle='-',c='b',label='10-h fuel truth')\n",
    "  plt.scatter(hour[:h2],data[:h2],c='b',label='10-h fuel data')\n",
    "  plt.plot(hour[:h2],m[:h2],linestyle='-',c='k',label='filtered')\n",
    "  plt.plot(hour[h2:hours],m[h2:hours],linestyle='-',c='r',label='filtered')\n",
    "  return E, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0EFhTPZAlBD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DeltaE = 0.0          # bias\n",
    "E, P = kf_example(DeltaE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqyB2Yz3uCsD"
   },
   "source": [
    "We have recovered the fuel moisture from data with random noise - we **filtered** the noise out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure() # new figure\n",
    "plt.plot(P,linestyle='-',c='b',label='Estimated state variance P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ccr-uKbmAlBE"
   },
   "source": [
    "Now what if the model is wrong - different from nature? That is always so in reality. Now suppose that the model and the truth are not the same. That is always the case in reality.  Consider a simple case when the model thinks that the equilibrium $E$ is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spMdGW8oAlBE"
   },
   "outputs": [],
   "source": [
    "DeltaE = -0.05\n",
    "E, P = kf_example(DeltaE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQeF7J8T4j2i"
   },
   "source": [
    "We have found a good estimate of the state $m$, while data is available. Also, the estimated state variance $P$ converges with time - we have *learned* the variance that balances the noise. But for forecasting fuel moisture, we need to continue the fuel moisture model into the future, and we can't have any measurements from future. We only have the equilibrium from weather forecast. And the forecast and the truth disagree - as soon as there is no data to attract the simulation, the model is doing its own thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uXVJj9koGF2"
   },
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel moisture RAWS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load the data from a saved file first. If that fails, retrieve the fuel moisture data from sensors on weather stations in the Mesowest network. Get all stations with fuel moisture data in a spatial box within one hour, then pick one station and retrieve the whole time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jfile = 'raws.json'\n",
    "try:\n",
    "    ! wget --no-clobber https://raw.githubusercontent.com/janmandel/notebooks/main/raws.json\n",
    "    j = json.load(open(jfile,'r'))\n",
    "    print('loaded from ',jfile)\n",
    "    # Take the first station in the boulding box that has data between time_start and time_s2.\n",
    "    # Then retrieve data for that station between time_start and time_end\n",
    "    time_start = j['time_start']      # start of data time series\n",
    "    # time_s2    = j['time_s2']         # end of segment to read coordinates\n",
    "    time_end  = j['time_end']         # end of data time series\n",
    "    meso_ts  = j['meso_ts']           # get meso observations time series\n",
    "    obs_lon =   j['obs_lon']          # where we retrieved observations\n",
    "    obs_lat =   j['obs_lat']\n",
    "except:\n",
    "    print(\"can't read\",jfile,', creating')\n",
    "    # set up bounds\n",
    "    time_start = \"201806010800\"  # June 1 2018 08:00 in format yyyymmddHHMM\n",
    "    time_s2    = \"201806010900\"  # June 1 2018 09:00 in format yyyymmddHHMM \n",
    "    time_end   = \"201906200900\"  # Nov 1 2018 09:00 in format yyyymmddHHMM \n",
    "    #time_start=  \"201810230100\"\n",
    "    #time_s2=  \"201810230300\"\n",
    "    #time_end  =  \"201806022300\"\n",
    "    !pip install MesoPy\n",
    "    from MesoPy import Meso\n",
    "    bounding_box = \"-115, 38, -110, 40\"  # min longtitude, latitude\n",
    "    meso_token=\"b40cb52cbdef43ef81329b84e8fd874f\"       # you should get your own if you do more of this\n",
    "    m = Meso(meso_token)# create a Meso object\n",
    "    print('reading MesoWest fuel moisture data from',)\n",
    "    meso_obss = m.timeseries(time_start, time_s2, bbox=bounding_box, \n",
    "                             showemptystations = '0', vars='fuel_moisture')   # ask the object for data\n",
    "    # pick one station and retrieve the whole time series.\n",
    "    station=meso_obss['STATION'][0]\n",
    "    #print(json.dumps(station, indent=4))\n",
    "    lon,lat = (float(station['LONGITUDE']),float(station['LATITUDE']))\n",
    "    print(station['NAME'],'station',station['STID'],'at',lon,lat)\n",
    "    e = 0.01   # tolerance\n",
    "    bb = '%s, %s, %s, %s' % (lon - e, lat - e, lon + e, lat + e)\n",
    "    print('bounding box',bb)\n",
    "    meso_ts = m.timeseries(time_start, time_end, bbox=bb, showemptystations = '0', vars='fuel_moisture')   # ask the object for data\n",
    "    obs_lon, obs_lat = (lon, lat)   # remember station coordinates for later\n",
    "    j={'time_start':time_start,'time_s2':time_s2,'time_end':time_end,\n",
    "       'meso_ts':meso_ts,'obs_lon':obs_lon,'obs_lat':obs_lat}\n",
    "    json.dump(j,open(jfile,'w'),indent=4)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bXopS3btyz0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process the data retrieved for this station\n",
    "# print(json.dumps(meso_ts['STATION'][0], indent=4))\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "station = meso_ts['STATION'][0]\n",
    "time_str  = station['OBSERVATIONS']['date_time']\n",
    "obs_time = [datetime.strptime(t, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=pytz.UTC) for t in time_str]\n",
    "start_time = obs_time[0].replace(minute=0)     # remember obs_time and start_time for later\n",
    "end_time = obs_time[-1]\n",
    "obs_data = np.array(station['OBSERVATIONS'][\"fuel_moisture_set_1\"])\n",
    "# display the data retrieved\n",
    "#for o_time,o_data in zip (obs_time,obs_data):\n",
    "#    print(o_time,o_data)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(obs_data,linestyle='-',c='k',label='10-h fuel data')\n",
    "plt.title(station['STID'] + ' 10 h fuel moisture data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY4hPeATK9wZ"
   },
   "source": [
    "## Retrieve weather analysis for the duration of the station data, from our RTMA stash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ-uJI2sy6I3"
   },
   "source": [
    "Create a function to transfer RTMA files in GRIB2 format from the stash. The function returns zero if the file transfer succeeded. If the file is not available, it returns a nonzero value. Note: if needed, maybe in future add more sophisticated checks, check the return code of wget and if the file size is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxZABVDxt0gd"
   },
   "outputs": [],
   "source": [
    "import subprocess,os\n",
    "def load_rtma(path,file,reload=0):\n",
    "  url='http://math.ucdenver.edu/~jmandel/rtma/' + path \n",
    "  if os.path.exists(file):\n",
    "    if reload:\n",
    "      print(file + ' already exists, removing')\n",
    "      os.remove(file)\n",
    "    else:\n",
    "      print(file + ' already exists, exiting')\n",
    "      # add checking size here\n",
    "      return 0\n",
    "  try:\n",
    "    ret = subprocess.check_output(['wget','--no-clobber','--output-document='+ file, url,],stderr=subprocess.STDOUT).decode() # execute command from python strings\n",
    "    if os.path.exists(file):\n",
    "      print('loaded ' + url + ' as ' + file)\n",
    "      return 0\n",
    "    else: \n",
    "      print('file transfer completed, but the file is missing? ' + url)  \n",
    "      return 1\n",
    "  except:\n",
    "    print('file transfer failed: ' + url)\n",
    "    return 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THI6gElyHOOc"
   },
   "source": [
    "Next, functions to get the files, open as grib, and interpolate to the station coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PL3gxK67AlBI"
   },
   "outputs": [],
   "source": [
    "def rtma_grib(t,var):\n",
    "    tpath = '%4i%02i%02i/%02i' % (t.year, t.month, t.day, t.hour)  # remote path on server\n",
    "    tstr  = '%4i%02i%02i%02i_' % (t.year, t.month, t.day, t.hour)  # time string for local path\n",
    "    gribfile = os.path.join('data',tstr + var + '.grib')\n",
    "    remote = tpath + '/' + var + '.grib'\n",
    "    if load_rtma(remote,gribfile):\n",
    "        print('cannot load remote file',remote,'as',gribfile)\n",
    "        return []\n",
    "    else:\n",
    "        try:\n",
    "            gf=GribFile(gribfile)\n",
    "            v = np.array(gf[1].values())\n",
    "        except:\n",
    "            print('cannot read grib file',gribfile)\n",
    "            return []\n",
    "        print('loaded ',gribfile,' containing array shape ',v.shape)\n",
    "        return gf[1]   # grib message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccp10kurAlBI"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator, interpn\n",
    "from scipy.optimize import root\n",
    "def interp_to_lat_lon_slow(lats,lons,v,lat,lon): \n",
    "    # on mesh with coordinates lats and lons interpolate v to given lat lon\n",
    "    interp=LinearNDInterpolator(list(zip(lats.flatten(),lons.flatten())),v.flatten())\n",
    "    return interp(lat,lon)\n",
    "def interp_to_lat_lon(lats,lons,v,lat,lon):\n",
    "    # on mesh with coordinates lats and lons interpolate v to given lat lon\n",
    "    points=(np.array(range(lats.shape[0]),float),np.array(range(lats.shape[1]),float))  # uniform mesh\n",
    "    def res(ij):  # interpolation of lons lats on the uniform mesh, to noninteger coordinates   \n",
    "       return np.hstack((interpn(points,lats,ij)-lat, interpn(points,lons,ij)-lon))\n",
    "    # solve for xi,xj such that lats(xi,xj)=lat lons(xi,xj)=lon, then interpolate to (xi, xj) on uniform grid \n",
    "    result = root(res,(0,0)) # solve res(ij) = 0\n",
    "    if not result.success:\n",
    "        print(result.message)\n",
    "        exit(1)\n",
    "    return interpn(points,v,result.x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvnpq6S5AlBI"
   },
   "source": [
    "The interpolation function needs to  be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVMJBYI7AlBI"
   },
   "outputs": [],
   "source": [
    "def interp_to_lat_lon_test(lats,lons):\n",
    "    print('testing interp_to_lat_lon')\n",
    "    vx, vy = np.meshgrid(range(lats.shape[0]),range(lats.shape[1]),indexing='ij')\n",
    "    i, j = (1,2)\n",
    "    lat,lon = ((lats[i,j]+lats[i+1,j+1])/2,(lons[i,j]+lons[i+1,j+1])/2)\n",
    "    vi = interp_to_lat_lon(lats,lons,vx,lat,lon)\n",
    "    vj = interp_to_lat_lon(lats,lons,vy,lat,lon)\n",
    "    print(vi,vj,'should be about',i+0.5,j+0.5)\n",
    "    test_slow = 0\n",
    "    if test_slow:\n",
    "        print('Testing against the standard slow method scipy.interpolate.LinearNDInterpolator. Please wait...')\n",
    "        vi_slow = interp_to_lat_lon_slow(lats,lons,vx,lat,lon)\n",
    "        print(vi_slow)\n",
    "        vj_slow = interp_to_lat_lon_slow(lats,lons,vy,lat,lon)\n",
    "        print(vj_slow)\n",
    "        \n",
    "#gf = rtma_grib(start_time,'temp')      #  read the first grib file and use it to test interpolation\n",
    "#lats, lons = gf.latlons()\n",
    "#interp_to_lat_lon_test(lats,lons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQbWB_3GAlBI"
   },
   "source": [
    "Now we are ready for a function to read the RTMA files and interpolate to the station coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3JJH3XPAlBI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "def read_interp_rtma(varnames,times,lat,lon):\n",
    "    # read RTMA from start_time to end_time and interpolate to obs_lat obs_lon\n",
    "    ntimes = len(times)\n",
    "    time_str = 'time_str'\n",
    "    j={time_str:times.strftime('%Y-%m-%d %H:%M').tolist()}\n",
    "    for varname in varnames:\n",
    "        j[varname]=np.full(ntimes,np.nan)  # initialize array of nans as list\n",
    "    n=0\n",
    "    for t in times:\n",
    "        tim=t.strftime('%Y-%m-%d %H:%M')\n",
    "        should_be = j[time_str][n]\n",
    "        if tim != should_be:\n",
    "            print('n=',n,'time',tim,'expected',should_be)\n",
    "            raise 'Invalid time' \n",
    "        for varname in varnames:\n",
    "            gf = rtma_grib(t,varname)   # read and create grib object, download if needed\n",
    "            if gf:\n",
    "                lats,lons = gf.latlons()    # coordinates\n",
    "                v = gf.values()\n",
    "                vi=interp_to_lat_lon(lats,lons,v,lat,lon) # append to array\n",
    "                print(varname,'at',t,'interpolated to',lat,lon,' value ',vi)\n",
    "                j[varname][n] = vi\n",
    "            else:\n",
    "                print(varname,'at',t,' could not be loaded')\n",
    "        n = n+1\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jfile = 'rtma.json'\n",
    "try:\n",
    "    j = json.load(open(jfile,'r'))\n",
    "    print('loaded from ',jfile)\n",
    "    if j['obs_lat']!=obs_lat or j['obs_lon']!=obs_lon:\n",
    "        raise 'Wrong lon lat'\n",
    "except:\n",
    "    print(\"can't read\",jfile,', creating')\n",
    "    # Set up environment to read RTMA gribs\n",
    "    # we will need current numpy for pygrib - needed on Colab, tensorflow is using numpy 1.19\\\n",
    "    times = pd.date_range(start=time_start,end=time_end,freq='1H')\n",
    "    varnames=['temp','td','precipa']\n",
    "    j =    read_interp_rtma(varnames,times,obs_lat,obs_lon)      # temperature\n",
    "    for varname in varnames:\n",
    "        j[varname]=j[varname].tolist() \n",
    "    j['obs_lat']=obs_lat\n",
    "    j['obs_lon']=obs_lon\n",
    "    json.dump(j,open('rtma.json','w'),indent=4)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtma = j\n",
    "td = np.array(rtma['td'])\n",
    "t2 = np.array(rtma['temp'])\n",
    "rain=np.array(rtma['precipa'])\n",
    "# compute relative humidity\n",
    "rh = 100*np.exp(17.625*243.04*(td - t2) / (243.04 + t2 - 273.15) / (243.0 + td - 273.15))\n",
    "Ed = 0.924*rh**0.679 + 0.000499*np.exp(0.1*rh) + 0.18*(21.1 + 273.15 - t2)*(1 - np.exp(-0.115*rh))\n",
    "Ew = 0.618*rh**0.753 + 0.000454*np.exp(0.1*rh) + 0.18*(21.1 + 273.15 - t2)*(1 - np.exp(-0.115*rh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZIK59bJAlBJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(t2,linestyle='-',c='k',label='Temperature')\n",
    "plt.title(station['STID'] + ' Temperature (K)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbyqcuXYAlBJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(td,linestyle='-',c='k',label='Dew point')\n",
    "plt.title(station['STID'] + ' Dew point (K)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(rh,linestyle='-',c='k',label='Dew point')\n",
    "plt.title(station['STID'] + ' relative humidity (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(Ed,linestyle='-',c='r',label='drying equilibrium')\n",
    "plt.plot(Ew,linestyle=':',c='b',label='wetting equilibrium')\n",
    "plt.title(station['STID'] + ' drying and wetting equilibria (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQKSRvRSAlBJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(rain,linestyle='-',c='k',label='Precipitation')\n",
    "plt.title(station['STID'] + ' Precipitation' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwbt4UXfro5x"
   },
   "outputs": [],
   "source": [
    "print(rain[1900:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yRu_7WvHc6P"
   },
   "source": [
    "Precipitation from RTMA is in kg/m${}^2$. 1m water depth over 1m${}^2$ is 1m${}^3$ with mass 1000 kg thus 1 kg/m${}^2$ is the same as 1 mm of precipitation. RTMA values are accumulations over 1 h so these are values in mm/h. So 9999 mm/h = 10m/h makes no sense. Replace anything over 1m/h by nan and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain[rain > 1000] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(rain,linestyle='-',c='k',label='Precipitation')\n",
    "plt.title(station['STID'] + ' Precipitation' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some missing data, then we can use the data for up to 1942 hours until a biger gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix isolated nans\n",
    "def fixnan(a,n):\n",
    "    for c in range(n):\n",
    "        for i in np.where(np.isnan(a)):\n",
    "            a[i]=0.5*(a[i-1]+a[i+1])\n",
    "        if not any(np.isnan(a)):\n",
    "            break\n",
    "    return a\n",
    "\n",
    "rain=fixnan(rain,2)\n",
    "t2=fixnan(t2,2)\n",
    "rh=fixnan(rh,2)\n",
    "obs_data=fixnan(obs_data,2)\n",
    "Ed=fixnan(Ed,2)\n",
    "Ew=fixnan(Ew,2)\n",
    "\n",
    "print(np.where(np.isnan(rain)))\n",
    "print(np.where(np.isnan(t2)))\n",
    "print(np.where(np.isnan(rh)))\n",
    "print(np.where(np.isnan(obs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SSWIbGCAlBK"
   },
   "outputs": [],
   "source": [
    "### Define model function with drying, wetting, and rain equilibria\n",
    "\n",
    "r0 = 0.05                                   # threshold rainfall [mm/h]\n",
    "rk = 8.0                                    # saturation rain intensity [mm/h]\n",
    "Trk = 14.0                                 # time constant for rain wetting model [h]\n",
    "S = 2.5                                     # saturation intensity [dimensionless]\n",
    "T = 10.0                                    # time constant for wetting/drying\n",
    "\n",
    "def model_moisture(m0,Eqd,Eqw,r,partials=0):\n",
    "    if r > r0:\n",
    "        E = S\n",
    "        T1 = 1.0 / (Trk * (1.0 - np.exp(- (r - r0) / rk)))\n",
    "    elif m0 > Eqd:\n",
    "        E = Eqd\n",
    "        T1 = 1.0/T\n",
    "    elif m0 < Eqw:\n",
    "        E = Eqw\n",
    "        T1 = 1.0/T\n",
    "    else:\n",
    "        E = m0\n",
    "        T1 = 1.0\n",
    "        \n",
    "    # model_decay(m0,E,partials=0,T1=0.1,tlen=1):  # T1 = 1/T, partials 0=none, 1=dm0/dm1, 2=dm1/dm0, dm1/dE, dm1/dT1     \n",
    "    return model_decay(m0,E,partials=partials,T1=T1,tlen=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter with RAWS observations, followed by forecasting\n",
    "We run the model first with Kalman filter for 150 hours. The observations are the RAWS data\n",
    "After 150 hours, we run in forecast mode - the RAWS data are no longer used, and we run the model from the weather data without the Kalman filter. The weather data are taken to be RTMA interpolated to one RAWS location.\n",
    "In a real forecasting application, the model would be run from weather forecast rather than data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run KF on an initial data seqment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "hours=300\n",
    "h2 = round(hours/2)\n",
    "m = np.zeros(hours) # preallocate\n",
    "m[0]= obs_data[0]             # initial state  \n",
    "P = np.zeros(hours)\n",
    "P[0] = 1e-3 # background state variance\n",
    "H = np.array([1.])   # all oQ = np.array([0.02]) # process noise variancebserved\n",
    "Q = np.array([1e-3]) # process noise variance\n",
    "R = np.array([1e-3]) # data variance\n",
    "for t in range(hours-1):\n",
    "    # use lambda construction to pass additional arguments to the model \n",
    "    if t < h2 and not np.isnan(obs_data[t]) and not np.isnan(Ew[t]) and not np.isnan(rain[t]): # advance model and run KF\n",
    "        m[t+1],P[t+1] = ext_kf(m[t],P[t],lambda u: model_moisture(u,Ed[t],Ew[t],rain[t],partials=1),Q,\n",
    "                    d=obs_data[t],H=H,R=R)\n",
    "    else:  # just advance to next hour\n",
    "        m[t+1],P[t+1] = ext_kf(m[t],P[t],lambda u: model_moisture(u,Ed[t],Ew[t],rain[t],partials=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(Ed[:hours],linestyle='--',c='r',label='Drying Equilibrium')\n",
    "plt.plot(Ew[:hours],linestyle='--',c='b',label='Wetting Equilibrium')\n",
    "plt.plot(obs_data[:hours],linestyle=':',c='k',label='RAWS data')\n",
    "plt.plot(m[:h2],linestyle='-',c='k',label='filtered')\n",
    "plt.plot(range(h2,hours),m[h2:hours],linestyle='-',c='r',label='forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the state variance $P$. $P\\to 0$ is a common problem in data assimilation, called filter degeneracy. With increasing cumulative amount of data, the variance of the state decreases and then the filter blindly follows the model. The underlying reason is that we trust that the model is accurate. This can work when nature and the model are the same system of equations, but here the nature is the nature and we need to recognize that the model is not accuratel. We guard against filter degeneracy by adding process noise $Q$ to the model. Ideally, the process noise variance  and the data noise variance should be about the same, then the assimilation will split the difference between the model uncertainly and the data uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure() # new figure\n",
    "plt.plot(P,linestyle='-',c='b',label='Estimated state variance P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is a problem - the forecast fuel moisture is too high. We need to assimilate also some parameters of the model, not just its state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jivOYEhiXMi5"
   },
   "source": [
    "## Model with augmented state\n",
    "In reality, the equilibrium moisture $E$ computed from atmospheric conditions\n",
    "generally does not agree with the data. We want to add a correction $\\Delta\n",
    "E$ to $E$ constant in time, and identify the new parameter $\\Delta E$ from data. \n",
    "Because the Kalman filter identifies state, add the parameter to the state.\n",
    "Define augmented state $u=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "m\\\\\n",
    "\\Delta E\n",
    "\\end{array}\n",
    "\\right]  .$ Since $\\Delta E$ is constant in time, it satisfies the\n",
    "differential equation $\\frac{d\\Delta E}{dt}=0.$ So, we want to estimate the\n",
    "state $u$ governed by the\n",
    "$$\n",
    "\\frac{d}{dt}\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "m\\\\\n",
    "\\Delta E\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\frac{E+\\Delta E-m(t)}{T}\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]  ,\n",
    "$$\n",
    "which we write as $\\frac{du}{dt}=F(u),$ where\n",
    "$$\n",
    "F(u)=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "F_{1}\\left(  u\\right)  \\\\\n",
    "F_{2}\\left(  u\\right)\n",
    "\\end{array}\n",
    "\\right]  =F\\left(  \\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "m\\\\\n",
    "\\Delta E\n",
    "\\end{array}\n",
    "\\right]  \\right)  =\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\left(  E+\\Delta E-m(t)\\right)  T_{1}\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]  ,\\quad T_{1}=\\frac{1}{T}.\n",
    "$$\n",
    "The Jacobian of $F$ is\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}\n",
    "[c]{cc}\n",
    "\\frac{\\partial F_{1}}{\\partial u_{1}} & \\frac{\\partial F_{1}}{\\partial u_{2}\n",
    "}\\\\\n",
    "\\frac{\\partial F_{2}}{\\partial u_{1}} & \\frac{\\partial F_{2}}{\\partial u_{2}}\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{cc}\n",
    "\\frac{\\partial m_{1}}{\\partial m_{0}} & \\frac{\\partial m_{1}}{\\partial E}\\\\\n",
    "\\frac{\\partial\\Delta E}{\\partial m_{0}} & \\frac{\\partial\\Delta E}\n",
    "{\\partial\\Delta E}\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{cc}\n",
    "\\frac{\\partial m_{1}}{\\partial m_{0}} & \\frac{\\partial m_{1}}{\\partial E}\\\\\n",
    "0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "Here is a function that implements the augmented model $F$. The input is\n",
    "$u_{0}$. The output is $u_{1}$ and the Jacobian $du_{1}/du_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define augmented model function with drying, wetting, and rain equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHtAaGp9WSHT"
   },
   "outputs": [],
   "source": [
    "def model_augmented(u0,E,T1,tlen=1):\n",
    "  # state u is the vector [m,dE] with dE correction to equilibrium\n",
    "  m0 = u0[0]  # decompose u0\n",
    "  dE = u0[1]\n",
    "  # reuse the decay model\n",
    "  m1, dm1_dm0, dm1_dE, dm1_dT1  = model_decay(m0,E + dE,T1,tlen=tlen)\n",
    "  u1 = np.array([m1,dE])\n",
    "  J = np.array([dm1_dm0, dm1_dE],\n",
    "               [0.     ,     1.])\n",
    "  return m0, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SuVNg8TsW4d"
   },
   "source": [
    "### Run the extended Kalman filter on the augmented model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1No3g6HyAEh_"
   },
   "outputs": [],
   "source": [
    "u = np.zeros((2,2*hours)\n",
    "u[:,0]=[0.1,0.1]             # background state  \n",
    "P = np.zeros(2,2,2*hours)\n",
    "P[:,:,0] = np.array([[0.03, 0.],\n",
    "                  [0.,    0.03]]) # background state covariance\n",
    "Q = np.array([[0.03, 0.],\n",
    "            [0,    0.03]]) # process noise covariance\n",
    "H = np.array([[1., 0.],\n",
    "             [0.,  .0]])   # first component observed\n",
    "R = np.array([0.02]) # data variance\n",
    "\n",
    "DeltaE = 0.05          # bias\n",
    "for t in range(hours):\n",
    "  # use lambda construction to pass additional arguments to the model \n",
    "  u[:,t+1],P[:,:,t+1] = ext_kf(m[:,t],d2(P[:,:,t]),lambda u: model_decay(u,E[t]+DeltaE,partials=1),Q,\n",
    "                    d=data[t],H=H,R=R)\n",
    "for t in range(hours,2*hours - 1):\n",
    "  u[:,t+1],P[:,:,t+1] = ext_kf(m[t],d2(P[t]),lambda u: model_decay(u,E[t]+DeltaE,partials=1))\n",
    "  \n",
    "    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(day,E,linestyle='--',c='r',label='Equilibrium')\n",
    "plt.plot(day,m_f,linestyle='-',c='k',label='10-h fuel truth')\n",
    "plt.scatter(day[0:hours],data[0:hours],c='b',label='10-h fuel data')\n",
    "plt.plot(day,m,linestyle='-',c='r',label='filtered')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESJQ8dWiiork"
   },
   "outputs": [],
   "source": [
    "DeltaE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJDcq4QvNhJI"
   },
   "outputs": [],
   "source": [
    "d=np.array([])\n",
    "if d:\n",
    "  print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4_pVyy_IOHm"
   },
   "outputs": [],
   "source": [
    "for d in range(24):\n",
    "  print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8tMN3qOx3IP"
   },
   "source": [
    "# With real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IAGpfAiFtRu"
   },
   "outputs": [],
   "source": [
    "! pip install intergrid\n",
    "from intergrid.intergrid import Intergrid  # docs https://pypi.org/project/intergrid/\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "start_date = date(2018,5,19)\n",
    "end_date = date(2020,6,1)\n",
    "for d in pd.date_range(start_date,end_date,freq=\"1h\"):\n",
    "    path = d.strftime(\"%Y%m%d/%H\")\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQhxXLtbC5mc"
   },
   "source": [
    "#Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IavCNH4AC23l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1.])\n",
    "b = np.array([2.])\n",
    "c  = a @ b\n",
    "print('a',a)\n",
    "print('b',b)\n",
    "print('c=a@b',c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvsbbv2XZ2Hd"
   },
   "source": [
    "# Testers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsOqvQk6ZXZV"
   },
   "outputs": [],
   "source": [
    "# a basic ext_kf test\n",
    "import numpy as np\n",
    "u = [1,\n",
    "     2]\n",
    "P = [[2 , -1],\n",
    "    [-1 , 2]]\n",
    "A = [ [1 ,2],\n",
    "      [3 ,4]]\n",
    "u = np.array(u)      \n",
    "Q = np.array([[1,0],[0,1]])\n",
    "A = np.array(A)\n",
    "def fun(u):\n",
    "  return A @ u, A\n",
    "F = lambda u: fun(u)\n",
    "H = [[1, 0],\n",
    "     [0, 1]]\n",
    "d = [2,\n",
    "    3]\n",
    "R = [[2, 0],\n",
    "    [0, 2]]\n",
    "H = np.array(H)      \n",
    "d = np.array(d)\n",
    "R = np.array(R)\n",
    "ua,Pa = ext_kf(u,P,F,Q)\n",
    "print('ua=',ua)\n",
    "print('Pa=',Pa)\n",
    "ua,Pa = ext_kf(u,P,F,Q,d,H,R)\n",
    "print('ua=',ua)\n",
    "print('Pa=',Pa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try to approximate Ed Ew by ML following https://www.tensorflow.org/guide/keras/sequential_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ML following \n",
    "first simple example following https://machinelearningmastery.com/neural-networks-are-function-approximators/\n",
    "We want to find approximate the function that maps (t2,rh) to (Ew,Ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "model.add(layers.Dense(2, activation=\"sigmoid\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"sigmoid\", name=\"layer2\"))\n",
    "model.add(layers.Dense(2, name=\"layer3\"))  # alone just linear regression\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and separately scale the input and output variables\n",
    "sz=500\n",
    "train=250\n",
    "t2s=t2[:sz].reshape(-1,1)\n",
    "rhs=rh[:sz].reshape(-1,1)\n",
    "Eds=Ed[:sz].reshape(-1,1)\n",
    "Ews=Ew[:sz].reshape(-1,1)\n",
    "print(t2s.shape,rhs.shape,Eds.shape,Ews.shape)\n",
    "print(min(t2s),max(t2s),min(rhs),max(rhs),min(Eds),max(Eds),min(Ews),max(Ews))\n",
    "scale_t2 = MinMaxScaler()\n",
    "t2_scaled = scale_t2.fit_transform(t2s)\n",
    "scale_rh = MinMaxScaler()\n",
    "rh_scaled = scale_rh.fit_transform(rhs)\n",
    "scale_Ed = MinMaxScaler()\n",
    "Ed_scaled = scale_Ed.fit_transform(Eds)\n",
    "scale_Ew = MinMaxScaler()\n",
    "Ew_scaled = scale_Ew.fit_transform(Ews)\n",
    "print(t2_scaled.shape,rh_scaled.shape,Ed_scaled.shape,Ew_scaled.shape)\n",
    "print(min(t2_scaled),max(t2_scaled),min(rh_scaled),max(rh_scaled),\n",
    "      min(Ed_scaled),max(Ed_scaled),min(Ew_scaled),max(Ew_scaled))\n",
    "x=np.concatenate([t2_scaled,rh_scaled],axis=1)\n",
    "y=np.concatenate([Ed_scaled,Ew_scaled],axis=1)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x[:,:train], y[:,:train], epochs=500, batch_size=10, verbose=0)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "y_pred = model.predict(x)\n",
    "# inverse transforms\n",
    "print(y_pred.shape)\n",
    "Ed_pred = scale_Ed.inverse_transform(y_pred[:,0].reshape(-1, 1))\n",
    "Ew_pred = scale_Ew.inverse_transform(y_pred[:,1].reshape(-1, 1))\n",
    "print(Ed_pred.shape,Ew_pred.shape)\n",
    "# report model error\n",
    "print('Ed min max:',min(Eds),max(Eds),'diff',max(Eds)-min(Eds) )\n",
    "print('Ed MSE on training data:   %.3f' % mean_squared_error(Eds[:train], Ed_pred[:train]))\n",
    "print('Ed MSE on validation data: %.3f' % mean_squared_error(Eds[train:], Ed_pred[train:]))\n",
    "print('Ew min max:',min(Ews),max(Ews),'diff',max(Ews)-min(Ews) )\n",
    "print('Ew MSE on training data:   %.3f' % mean_squared_error(Ews[:train], Ew_pred[:train]))\n",
    "print('Ew MSE on validation data: %.3f' % mean_squared_error(Ews[train:], Ew_pred[train:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data (labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs_data[:sz].reshape(-1,1)\n",
    "scale_obs = MinMaxScaler()\n",
    "obs_scaled = scale_obs.fit_transform(Eds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128,input_shape=(3,))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jivOYEhiXMi5",
    "Uvsbbv2XZ2Hd"
   ],
   "name": "fmda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

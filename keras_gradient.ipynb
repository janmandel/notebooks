{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boe2aBjhQ0ug"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q_8h7rvQHb8"
   },
   "source": [
    "# Keras gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR0_pDfSReZw"
   },
   "source": [
    "## How to replace Keras' gradients() function with GradientTape in TF2.0?\n",
    "\n",
    "\n",
    "From https://github.com/tensorflow/tensorflow/issues/33135 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaBZjNH4R83R",
    "outputId": "c748539d-7b0c-4879-b316-754d64e2b448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 17:18:22.473212: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-03 17:18:22.474125: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgpHen0nSVKY",
    "outputId": "4c760261-839b-4ad8-e10a-9beca7815430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= [[1.]] grad(w*w)= [[2.]]\n"
     ]
    }
   ],
   "source": [
    "print('w=',w.numpy(),'grad(w*w)=',grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R9YO4qwU-Sw"
   },
   "source": [
    "### Try this on a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW_2Zp4lZyrl"
   },
   "source": [
    "## Using TensorFlow and GradientTape to train a Keras model\n",
    "\n",
    "From https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uKOPEo3HZ8fb"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cdjp8uLuaZ6O"
   },
   "outputs": [],
   "source": [
    "def build_model(width, height, depth, classes):\n",
    "\t# initialize the input shape and channels dimension to be\n",
    "\t# \"channels last\" ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\t# build the model using Keras' Sequential API\n",
    "\tmodel = Sequential([\n",
    "\t\t# CONV => RELU => BN => POOL layer set\n",
    "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tFlatten(),\n",
    "\t\tDense(256),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(),\n",
    "\t\tDropout(0.5),\n",
    "\t\t# softmax classifier\n",
    "\t\tDense(classes),\n",
    "\t\tActivation(\"softmax\")\n",
    "\t])\n",
    "\t# return the built model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gnc5j4NTaq5Z"
   },
   "outputs": [],
   "source": [
    "def step(X, y):\n",
    "\t# keep track of our gradients\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\t# make a prediction using the model and then calculate the\n",
    "\t\t# loss\n",
    "\t\tpred = model(X)\n",
    "\t\tloss = categorical_crossentropy(y, pred)\n",
    "\t# calculate the gradients using our tape and then update the\n",
    "\t# model weights\n",
    "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
    "\topt.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kU8chpCQa3pk",
    "outputId": "ddf1d8bc-9d5b-43f1-ac41-e6efede88d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "# one-hot encode the labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiEzKDxVbD91",
    "outputId": "a82f3977-3612-4a1c-9acb-a4d24a96babb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating model...\n"
     ]
    }
   ],
   "source": [
    "# build our model and initialize our optimizer\n",
    "print(\"[INFO] creating model...\")\n",
    "model = build_model(28, 28, 1, 10)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SJvm-cGbKxw",
    "outputId": "5e9eded1-29c3-4744-84e5-daab12853dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting epoch 1/25...took 4.173 minutes\n",
      "[INFO] starting epoch 2/25...took 4.545 minutes\n",
      "[INFO] starting epoch 3/25...took 11.5 minutes\n",
      "[INFO] starting epoch 4/25...took 4.231 minutes\n",
      "[INFO] starting epoch 5/25...took 3.618 minutes\n",
      "[INFO] starting epoch 6/25...took 3.622 minutes\n",
      "[INFO] starting epoch 7/25...took 3.667 minutes\n",
      "[INFO] starting epoch 8/25...took 3.771 minutes\n",
      "[INFO] starting epoch 9/25...took 3.705 minutes\n",
      "[INFO] starting epoch 10/25...took 3.811 minutes\n",
      "[INFO] starting epoch 11/25...took 3.957 minutes\n",
      "[INFO] starting epoch 12/25...took 4.131 minutes\n",
      "[INFO] starting epoch 13/25...took 3.877 minutes\n",
      "[INFO] starting epoch 14/25...took 3.964 minutes\n",
      "[INFO] starting epoch 15/25...took 4.031 minutes\n",
      "[INFO] starting epoch 16/25...took 4.076 minutes\n",
      "[INFO] starting epoch 17/25...took 4.089 minutes\n",
      "[INFO] starting epoch 18/25...took 4.2 minutes\n",
      "[INFO] starting epoch 19/25...took 4.032 minutes\n",
      "[INFO] starting epoch 20/25...took 4.307 minutes\n",
      "[INFO] starting epoch 21/25...took 4.177 minutes\n",
      "[INFO] starting epoch 22/25...took 4.057 minutes\n",
      "[INFO] starting epoch 23/25...took 4.104 minutes\n",
      "[INFO] starting epoch 24/25...took 4.302 minutes\n",
      "[INFO] starting epoch 25/25...took 4.286 minutes\n"
     ]
    }
   ],
   "source": [
    "# compute the number of batch updates per epoch\n",
    "numUpdates = int(trainX.shape[0] / BS)\n",
    "# loop over the number of epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "\t# show the current epoch number\n",
    "\tprint(\"[INFO] starting epoch {}/{}...\".format(\n",
    "\t\tepoch + 1, EPOCHS), end=\"\")\n",
    "\tsys.stdout.flush()\n",
    "\tepochStart = time.time()\n",
    "\t# loop over the data in batch size increments\n",
    "\tfor i in range(0, numUpdates):\n",
    "\t\t# determine starting and ending slice indexes for the current\n",
    "\t\t# batch\n",
    "\t\tstart = i * BS\n",
    "\t\tend = start + BS\n",
    "\t\t# take a step\n",
    "\t\tstep(trainX[start:end], trainY[start:end])\n",
    "\t# show timing information for the epoch\n",
    "\tepochEnd = time.time()\n",
    "\telapsed = (epochEnd - epochStart) / 60.0\n",
    "\tprint(\"took {:.4} minutes\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edbEVGzzQyQU"
   },
   "source": [
    "## How to obtain the gradients in keras?\n",
    "From https://stackoverflow.com/questions/51140950/how-to-obtain-the-gradients-in-keras\n",
    " \n",
    "Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uGCC_a_jQFxs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(20, input_shape = (10, )))\n",
    "model.add(keras.layers.Dense(5))\n",
    "model.compile('adam', 'mse')\n",
    "\n",
    "dummy_in = np.ones((4, 10))\n",
    "dummy_out = np.ones((4, 5))\n",
    "dummy_loss = model.train_on_batch(dummy_in, dummy_out)\n",
    "\n",
    "def get_weight_grad(model, inputs, outputs):\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "\n",
    "def get_layer_output_grad(model, inputs, outputs, layer=-1):\n",
    "    \"\"\" Gets gradient a layer output for given inputs and outputs\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.layers[layer].output)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "\n",
    "weight_grads = get_weight_grad(model, dummy_in, dummy_out)\n",
    "output_grad = get_layer_output_grad(model, dummy_in, dummy_out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "keras gradient.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

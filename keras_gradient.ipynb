{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras gradient.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "boe2aBjhQ0ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras gradient\n"
      ],
      "metadata": {
        "id": "9q_8h7rvQHb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to replace Keras' gradients() function with GradientTape in TF2.0?\n",
        "\n",
        "\n",
        "From https://github.com/tensorflow/tensorflow/issues/33135 "
      ],
      "metadata": {
        "id": "iR0_pDfSReZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "w = tf.Variable([[1.0]])\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = w * w\n",
        "\n",
        "grad = tape.gradient(loss, w)\n",
        "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaBZjNH4R83R",
        "outputId": "c748539d-7b0c-4879-b316-754d64e2b448"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('w=',w.numpy(),'grad(w*w)=',grad.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgpHen0nSVKY",
        "outputId": "4c760261-839b-4ad8-e10a-9beca7815430"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w= [[1.]] grad(w*w)= [[2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try this on a model"
      ],
      "metadata": {
        "id": "-R9YO4qwU-Sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TensorFlow and GradientTape to train a Keras model\n",
        "\n",
        "From https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/"
      ],
      "metadata": {
        "id": "cW_2Zp4lZyrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "uKOPEo3HZ8fb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(width, height, depth, classes):\n",
        "\t# initialize the input shape and channels dimension to be\n",
        "\t# \"channels last\" ordering\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\t# build the model using Keras' Sequential API\n",
        "\tmodel = Sequential([\n",
        "\t\t# CONV => RELU => BN => POOL layer set\n",
        "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tFlatten(),\n",
        "\t\tDense(256),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(),\n",
        "\t\tDropout(0.5),\n",
        "\t\t# softmax classifier\n",
        "\t\tDense(classes),\n",
        "\t\tActivation(\"softmax\")\n",
        "\t])\n",
        "\t# return the built model to the calling function\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "Cdjp8uLuaZ6O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(X, y):\n",
        "\t# keep track of our gradients\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\t# make a prediction using the model and then calculate the\n",
        "\t\t# loss\n",
        "\t\tpred = model(X)\n",
        "\t\tloss = categorical_crossentropy(y, pred)\n",
        "\t# calculate the gradients using our tape and then update the\n",
        "\t# model weights\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\topt.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "Gnc5j4NTaq5Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the number of epochs to train for, batch size, and\n",
        "# initial learning rate\n",
        "EPOCHS = 25\n",
        "BS = 64\n",
        "INIT_LR = 1e-3\n",
        "# load the MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "# one-hot encode the labels\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU8chpCQa3pk",
        "outputId": "ddf1d8bc-9d5b-43f1-ac41-e6efede88d84"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading MNIST dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build our model and initialize our optimizer\n",
        "print(\"[INFO] creating model...\")\n",
        "model = build_model(28, 28, 1, 10)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiEzKDxVbD91",
        "outputId": "a82f3977-3612-4a1c-9acb-a4d24a96babb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the number of batch updates per epoch\n",
        "numUpdates = int(trainX.shape[0] / BS)\n",
        "# loop over the number of epochs\n",
        "for epoch in range(0, EPOCHS):\n",
        "\t# show the current epoch number\n",
        "\tprint(\"[INFO] starting epoch {}/{}...\".format(\n",
        "\t\tepoch + 1, EPOCHS), end=\"\")\n",
        "\tsys.stdout.flush()\n",
        "\tepochStart = time.time()\n",
        "\t# loop over the data in batch size increments\n",
        "\tfor i in range(0, numUpdates):\n",
        "\t\t# determine starting and ending slice indexes for the current\n",
        "\t\t# batch\n",
        "\t\tstart = i * BS\n",
        "\t\tend = start + BS\n",
        "\t\t# take a step\n",
        "\t\tstep(trainX[start:end], trainY[start:end])\n",
        "\t# show timing information for the epoch\n",
        "\tepochEnd = time.time()\n",
        "\telapsed = (epochEnd - epochStart) / 60.0\n",
        "\tprint(\"took {:.4} minutes\".format(elapsed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SJvm-cGbKxw",
        "outputId": "5e9eded1-29c3-4744-84e5-daab12853dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] starting epoch 1/25...took 2.757 minutes\n",
            "[INFO] starting epoch 2/25..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to obtain the gradients in keras?\n",
        "From https://stackoverflow.com/questions/51140950/how-to-obtain-the-gradients-in-keras\n",
        " \n",
        "Not working"
      ],
      "metadata": {
        "id": "edbEVGzzQyQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGCC_a_jQFxs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(20, input_shape = (10, )))\n",
        "model.add(keras.layers.Dense(5))\n",
        "model.compile('adam', 'mse')\n",
        "\n",
        "dummy_in = np.ones((4, 10))\n",
        "dummy_out = np.ones((4, 5))\n",
        "dummy_loss = model.train_on_batch(dummy_in, dummy_out)\n",
        "\n",
        "def get_weight_grad(model, inputs, outputs):\n",
        "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
        "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
        "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
        "    f = K.function(symb_inputs, grads)\n",
        "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
        "    output_grad = f(x + y + sample_weight)\n",
        "    return output_grad\n",
        "\n",
        "\n",
        "def get_layer_output_grad(model, inputs, outputs, layer=-1):\n",
        "    \"\"\" Gets gradient a layer output for given inputs and outputs\"\"\"\n",
        "    grads = model.optimizer.get_gradients(model.total_loss, model.layers[layer].output)\n",
        "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
        "    f = K.function(symb_inputs, grads)\n",
        "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
        "    output_grad = f(x + y + sample_weight)\n",
        "    return output_grad\n",
        "\n",
        "\n",
        "weight_grads = get_weight_grad(model, dummy_in, dummy_out)\n",
        "output_grad = get_layer_output_grad(model, dummy_in, dummy_out)"
      ]
    }
  ]
}
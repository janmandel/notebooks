\documentclass[12pt]{article}%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{color}
\usepackage{hyperref}%

\begin{document}

We present the Kalman filter in perhaps the most used form, as extended to nonlinear models.
 Consider a discrete time model of some natural
process. At time step $k$, the model has state $u_{k}\in\mathbb{R}^{n}$, which
can be approximated from the previous step $u_{k-1}$ by applying the model
$\mathcal{M}$ to get a forecast $u_{k}^{f}=\mathcal{M}\left(  u_{k-1}\right)
$. We model uncertainty in the model itself by adding normally distributed
noise with mean zero and covariance $Q$ to the uncertainty of $u_{k}^{f}$. We
also need to estimate now the uncertainty in the previous state $u_{k-1}$
propagates to the uncertainty of the forecast $u_{k}^{f}$. So, assume that the
model is differentiable and quantify the uncertainty of the state by a
covariance matrix. That is,  assume that at step $k-1$, the state has
(approximately) normal distribution with mean $u_{k-1}$ and covariance
$P_{k-1}$. Using the Taylor expansion of order $1$ of the model operator at
$u_{k-1}$, $\mathcal{M}\left(  u\right)  \approx\mathcal{M}\left(
u_{k-1}\right)  +\mathcal{M}^{\prime}\left(  u_{k-1}\right)  \left(
u-u_{k-1}\right)  $, where $\mathcal{M}^{\prime}\left(  u_{k-1}\right)  $ is
the Jacobian matrix of $\mathcal{M}$ at $u_{k-1}$. It can be shown that the
forecast has then (approximately)\ normal distribution with mean and
covariance
$$
u_{k}^{f}=\mathcal{M}\left(  u_{k-1}\right)  ,\ P_{k}^{f}=\mathcal{M}\left(
u_{k-1}\right)  P_{k-1}\mathcal{M}^{\prime}\left(  u_{k-1}\right)  +Q
$$
At time $k$, we also have an observation $d_{k}\approx Hu_{k}$, where $H$ is a
given observation operator, and we want to find $u_{k}$ so that both
$$
u_{k}\approx u_{k}^{f}\text{ and }d_{k}\approx Hu_{k}.
$$
We quantify the uncertainly of the error of observation $d_{k}$ by a covariance
matrix $R$: assume that the observation error has normal probability
distribution with a known covariance $R$. Then, the likelihood of state $u$ is
proportional to $e^{-\left\Vert d_{k}-Hu\right\Vert _{R^{-1}}^{2}/2}$, where
we used the notation for the norm $\left\Vert v\right\Vert _{A}%
=\left(v^{\top}Av\right)^{1/2}$ induced by a positive definite matrix $A$. Similarly, we quantify the
uncertainty of the state by a covariance matrix $P_{k}$. That is, the forecast
state has (approximately) normal distribution with mean $u_{k}^{f}$  and covariance
$P_{k}^{f}$. From the Bayes theorem of statistics, the probability distribution
of the state after taking the data into account has density%
$$
p_{k}\left(  u\right) \propto e^\frac{-\left\Vert d_{k}
-Hu\right\Vert_{R^{-1}}^{2}}{2}e^\frac{-\left\Vert u-u_{k}^{f}\right\Vert _{
{P_{k}^f}^{-1}  }^{2}}{2}%
$$
where $\propto$ means proportional.
Note that the probability density at $u$ is maximal when $\left\Vert
d_{k}-Hu\right\Vert _{R^{-1}}^{2}+\left\Vert u-u_{k}\right\Vert _{{P_{k}^{f}}^{-1}}^{2}$
 is minimal, which quantifies the statement that $d_{k}\approx
Hu_{k}$ and $u\approx u_{k}^{f}$.  By a direct computation completing the
square and using the Sherman-Morrison-Woodbury formula, 
$$p_{k}\left(
		u
	   \right) \propto 
e^{-\frac{
	\left\Vert u-u_{k
	         }
	\right\Vert_
		{P_{k
		      }^{-1}
		}^{2}
	}
	{2}},
$$ 
which is the density of the normal distribution with the mean
$$
u_{k}^{f}=u_{k}^{f}+K_{k}(d-Hu_{k}^{f}),\ \text{where }K_{k}=P_{k}%
^{f}H^{\mathrm{T}}(HP_{k}^{f}H^{\mathrm{T}}+R)^{-1}%
$$
and covariance
$$
P_{k}=\left(  \left(  P_{k}^{f}\right)  ^{-1}+H^{\mathrm{T}}R^{-1}H\right)
^{-1}=(I-KH)P_{k}^{f}.
$$

These are the equations of the extended Kalman filter. The original Kalman (1960) filter was
formulated for a linear process. The extension to the
nonlinear case made broad array of applications possible, including the Apollo spacecraft naviation (McGee and Schmidt, 1966),  and is
still a de-facto standard in navigation and GPS.

\end{document}